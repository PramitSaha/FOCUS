<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="F3OCUS: Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating via Multi-objective Meta-Heuristics">
  <meta property="og:title" content="F3OCUS - CVPR 2025" />
  <meta property="og:description" content="Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating via Multi-objective Meta-Heuristics" />
  <meta property="og:url" content="https://pramitsaha.github.io/f3ocus-cvpr2025/" />
  <meta property="og:image" content="static/images/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />
  <meta name="twitter:title" content="F3OCUS - CVPR 2025">
  <meta name="twitter:description" content="Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating via Multi-objective Meta-Heuristics">
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <meta name="keywords" content="Federated Learning, Vision-Language Models, Foundation Models, Meta-Heuristics, CVPR 2025">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>F3OCUS | CVPR 2025</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">
  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js" async></script>

</head>

<body>

<!-- Hero Section -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">
            F<sup>3</sup>OCUS: Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics
          </h1>
          <div class="is-size-5 publication-authors" style="margin-bottom: 0.5em;">
            <span class="author-block"><a href="https://pramitsaha.github.io/" target="_blank">Pramit Saha</a><sup>1*</sup></span>
            <span class="author-block"><a href="https://felixwagner.net/" target="_blank">Felix Wagner</a><sup>1</sup></span>
            <span class="author-block"><a href="https://mdivyanshu97.github.io/" target="_blank">Divyanshu Mishra</a><sup>1</sup></span>
            <span class="author-block"><a href="https://ibme.ox.ac.uk/person/can-peng/" target="_blank">Can Peng</a><sup>1</sup></span>
            <span class="author-block"><a href="https://eng.ox.ac.uk/people/anshul-thakur/" target="_blank">Anshul Thakur</a><sup>1</sup></span>
            <span class="author-block"><a href="https://ibme.ox.ac.uk/person/david-clifton/" target="_blank">David A. Clifton</a><sup>1,2</sup></span>
            <span class="author-block"><a href="https://ibme.ox.ac.uk/person/konstantinos-kamnitsas/" target="_blank">Konstantinos Kamnitsas</a><sup>1,3,4</sup></span>
            <span class="author-block"><a href="https://ibme.ox.ac.uk/person/alison-noble/" target="_blank">J. Alison Noble</a><sup>1</sup></span>
          </div>
          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Department of Engineering Science, University of Oxford<br>
              <sup>2</sup>Oxford-Suzhou Centre for Advanced Research<br>
              <sup>3</sup>Imperial College London<br>
              <sup>4</sup>University of Birmingham<br>
            </span>
            <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span><br>
            <span class="author-block" style="background: linear-gradient(to right, rgb(255, 0, 0), rgb(0, 255, 0), rgb(0, 0, 0)); -webkit-background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;"><b>CVPR 2025</b></span>
          </div>
          <div class="publication-links" style="margin-top: 1em;">
            <span class="link-block">
              <a href="https://arxiv.org/abs/2411.11912" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="ai ai-arxiv"></i></span>
                <span>ArXiv</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://openaccess.thecvf.com/content/CVPR2025/papers/Saha_F3OCUS_-_Federated_Finetuning_of_Vision-Language_Foundation_Models_with_Optimal_CVPR_2025_paper.pdf" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fas fa-file-pdf"></i></span>
                <span>Paper</span>
              </a>
            </span>
            <span class="link-block">
              <a href="https://github.com/PramitSaha/FOCUS-CVPR-2025" target="_blank" class="external-link button is-normal is-rounded is-dark">
                <span class="icon"><i class="fab fa-github"></i></span>
                <span>Code/Data</span>
              </a>
            </span>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser Figure -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <h4 class="subtitle"><b>Overview of F<sup>3</sup>OCUS</b></h4>
      <img src="static/images/CVPR 2025 (16) (1).png" alt="F3OCUS overview" width="700" style="margin: auto;" />
    </div>
  </div>
</section>

<!-- Highlights -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
          Highlights</h2>
      </div>
    </div>
    <p>
      🌟 <b>Parameter-Efficient Finetuning (PEFT)</b> of Vision-Language Models (VLMs) for resource-constrained Federated Learning.<br>
      🌟 Uses <b>principal eigenvalue of layerwise Neural Tangent Kernels (NTKs)</b> as client-specific layer importance score.<br>
      🌟 <b>Distributed layer participation:</b> Encourages diverse layer selection across clients for optimal performance.<br>
      🌟 <b>Joint optimization:</b> Uses multi-objective meta-heuristics on the server to optimize both importance and diversity.<br>
      🌟 <b>MedVQA-FL Dataset:</b> 707,962 VQA triplets, 9 modality-specific clients, 12 anatomical categories.<br>
      🌟 <b>Extensive Experiments:</b> 10,000+ client-level runs, 6 FL settings, 58 datasets, 4 VLMs.<br>
    </p>
  </div>
</section>

<!-- Motivation -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
          Motivation</h2>
        <h5 class="title is-5">How can we efficiently and fairly fine-tune massive foundation models for real-world FL in domains with strict privacy and limited resources?</h5>
        <div class="columns is-centered">
          <div class="column">
            <div class="content">
              <h4 class="title is-5" style="text-align: center;">🔒 Challenges</h4>
              <p>
                (1) <b>Resource Constraints:</b> Fine-tuning foundation models exceeds client resources.<br>
                (2) <b>Layer Selection:</b> Naive selection overfits and ignores client-specific needs.<br>
                (3) <b>Competing Goals:</b> Balancing client adaptation and collaborative layer usage is hard.<br>
              </p>
            </div>
          </div>
          <div class="column">
            <div class="content">
              <h4 class="title is-5" style="text-align: center;">🔑 Solutions</h4>
              <p>
                <b>Client Layer Importance:</b> NTK spectral analysis scores each layer per client.<br>
                <b>Server-Side Layer Balancing:</b> Meta-heuristics maximize adaptation and distribute layer updates.<br>
                <b>Privacy-Preserving:</b> Optimization happens without sharing raw data.<br>
              </p>
            </div>
          </div>
        </div>
        <div class="hero-body has-text-centered">
          <h4 class="subtitle"><b>Distinction from prior works</b></h4>
          <img src="static/images/CVPR 2025 (1) (2).png" alt="Approach comparison" width="900" style="margin: auto;" />
          <p style="margin-top:1em;">
            (a) Vanilla: selects layers only on local client data, neglecting collaboration.<br>
            (b) <b>F<sup>3</sup>OCUS:</b> jointly maximizes client-specific layer importance and distributes layer adaptation across clients.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


  <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
          Client-level Layer Importance
        </h2>

        <h4 class="subtitle">Layerwise Neural Tangent Kernel (LNTK)</h4>

        <div class="content" style="text-align: left; margin: auto; max-width: 800px;">
          <p>
            We use the <b>Layerwise Neural Tangent Kernel (LNTK)</b> to score which layers should be adapted for each client. The LNTK captures how sensitive the model’s output is to each layer's parameters on a client’s data.
          </p>
          <p>
            For neural network function \(f\) trained on client data \(\mathcal{X}\):
          </p>
          <p>
            $$
            \dot{f} = -\eta \Theta(\mathcal{X}, \mathcal{X}) \nabla_{f} F (\mathcal{X}; \theta_t)
            $$
          </p>
          <p>
            The NTK matrix:
          </p>
          <p>
            $$
            \Theta(\mathcal{X}, \mathcal{X}) = \nabla_{\theta} f(\mathcal{X}; \theta_t) \nabla_{\theta} f(\mathcal{X}; \theta_t)^T
            $$
          </p>
          <p>
            ...can be split into per-layer (LNTK) components:
          </p>
          <p>
            $$
            \Theta(\mathcal{X}, \mathcal{X}) = \sum_{l=1}^L \Theta^l(\mathcal{X}, \mathcal{X})
            $$
            $$
            \Theta^l(\mathcal{X}, \mathcal{X}) = \nabla_{\theta^l} f (\mathcal{X}; \theta^l) \nabla_{\theta^l} f(\mathcal{X}; \theta^l)^T
            $$
          </p>
          <p>
            The <b>principal eigenvalue</b> \(\lambda^l_1\) of each layer’s LNTK determines how quickly that layer learns and aligns with the client’s data.
          </p>

          <figure style="text-align: center;">
            <img src="static/images/CVPR 2025 (3) (2).png" width="2800" alt="Layerwise LNTK ranks illustration"/>
            <figcaption style="text-align: center;">LNTK-based layer ranks across clients/rounds. Darker = higher importance.</figcaption>
          </figure>

          <p>
            We define the <b>importance score</b> for layer \(l\) on client \(i\) as:
          </p>
          <p>
            $$
            S^l_i = \frac{\lambda^l_{i, 1}}{\sum_{k=1}^L \lambda^k_{i, 1}}
            $$
          </p>
          <p>
            This enables selective, efficient adaptation of the most relevant layers for each client.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
          Server-side Multi-objective Optimization
        </h2>
        <h4 class="subtitle">Balancing Adaptation and Layer Diversity</h4>
         <figure style="text-align: center;">
            <img src="static/images/CVPR 2025 (7) (2).png" width="80%" alt="F<sup>3</sup>OCUS ranks illustration"/>
            <figcaption style="text-align: center;">Multi-objective Optimization-based layer ranks across clients/rounds. Darker = higher importance.</figcaption>
          </figure>

        <div class="content" style="text-align: left; margin: auto; max-width: 800px;">
          <p>
            On the server, we refine layer selection to jointly <b>maximize adaptation</b> (via client-specific importance) and <b>balance layer usage</b> across clients. We formalize this as a multi-objective problem:
          </p>
          <p>
            <b>Objectives:</b>
            <ul>
              <li>Maximize total client importance: \( \sum_{i=1}^N \sum_{l=1}^L S_i^l \)</li>
              <li>Minimize variance in layer usage: \( \frac{1}{L} \sum_{l=1}^L (n_l - \bar{n})^2 \), where \( n_l \) is the number of clients choosing layer \( l \) and \( \bar{n} \) is the average.</li>
            </ul>
          </p>
          <p>
            <b>Constraint:</b> Each client can only select up to their own budget of layers.
          </p>
          <p style="text-align: center;">
            $$
            \begin{cases}
              \mathbf{max} & \sum\limits_{i=1}^{N} \sum\limits_{l=1}^L S_{i}^{l} \\
              \mathbf{min} & \frac{1}{L} \sum\limits_{l=1}^{L} ( n_l - \bar{n} )^2 \\
              \text{s.t.} & \sum_{l \in \mathcal{L}_i} m_{i}^{l} \leq L_{\text{i,max}}, \ \forall i
            \end{cases}
            $$
          </p>
          <figure style="text-align: center;">
            <img src="static/images/CVPR 2025 (10) (1).png" width="100%" alt="Layer selection histogram"/>
            <figcaption style="text-align: center;">
              Server-level optimization promotes diverse layer participation and avoids overfitting to a few layers.
            </figcaption>
          </figure>
          <p>
            <b>Why Meta-heuristics?</b> Traditional methods struggle as the search space is huge and no client data is available on the server. We use five representative meta-heuristic algorithms to find a set of optimal trade-offs:
          </p>
          <ul>
            <li><b>NSGA-II:</b> Evolves layer selections via crossover/mutation; balances exploration and exploitation using non-dominated sorting and crowding distance.</li>
            <li><b>Artificial Bee Colony (ABC):</b> Bees represent candidate selections, with phases for exploring, exploiting, and diversifying solutions based on importance and diversity.</li>
            <li><b>Ant Colony Optimization (ACO):</b> Ants build layer assignments using pheromone trails and importance scores, encouraging exploration and reinforcing successful patterns.</li>
            <li><b>Simulated Annealing (SA):</b> Explores layer assignments via probabilistic acceptance, gradually refining towards balanced, high-importance solutions as the "temperature" cools.</li>
            <li><b>Multi-Objective Particle Swarm Optimization (MOPSO):</b> Particles update assignments by combining their own best with globally best solutions, balancing adaptation and diversity.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>



<!-- Dataset Section: Ultra-MedVQA -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); 
          -webkit-background-clip: text; -webkit-text-fill-color: transparent;line-height: 1.5;">
          Ultra-MedVQA Dataset
        </h2>

        <img src="static/images/CVPR 2025 (14) (2).png" alt="Ultra-MedVQA overview" width="2800" style="margin: auto;" />

        <div class="content" style="text-align: left; margin: auto; max-width: 800px; margin-top:1.5em;">
          <p>
            <b>Ultra-MedVQA</b> is a large-scale, federated medical VQA benchmark featuring <b>707,962 image–QA triplets</b> across 9 imaging modalities and 12 anatomical categories, distributed as 9 simulated clients. It maximizes real medical image utility and supports robust federated VQA evaluation.
          </p>

          <ol>
            <li>
              <b>Preparation of Original Dataset:</b>
              <br>
              Compiled from <b>10 diverse medical datasets</b> representing <b>Chest X-ray</b> (117,976), <b>Optical Coherence Tomography</b> (109,309), <b>Colon Pathology</b> (107,180), <b>Dermatoscope</b> (10,015), <b>Fundus</b> (1,600), <b>Ultrasound</b> (780), <b>Blood Microscope</b> (17,092), <b>Kidney Cortex Microscope</b> (236,386), <b>Abdominal CT</b> (107,624). Each dataset acts as a modality-specific client, split 80% train / 20% test.
            </li>
            <li>
              <b>Question-Answer Template Design:</b>
              <br>
              Category and attribute labels are programmatically converted to QA pairs using structured templates. Example: <i>“Q: What is the specific diagnosis for the lung in this image? A: Pneumothorax.”</i> Questions also cover modality, anatomy, and tissue localization.
              <ul style="margin-top:0.5em;">
                <li><b>Question types:</b> Modality Recognition (~10%), Anatomy Identification (~20%), Disease Diagnosis (~39%), Disease Grading (~1%), Tissue ID (~20%), Other Biological Attributes (~10%).</li>
              </ul>
            </li>
            <li>
              <b>Question-Answer Refinement:</b>
              <br>
              To maximize natural diversity, each question is paraphrased using ChatGPT-4o for varied style and expression, while preserving meaning.
            </li>
            <li>
              <b>Manual Double Checking:</b>
              <br>
              All samples were further reviewed to ensure QA pair correctness and overall dataset quality.
            </li>
          </ol>

          <p style="margin-top:1em;">
            <b>Ultra-MedVQA</b> provides a challenging, multi-modal federated VQA setting with fine-grained evaluation across anatomical regions (Colon, Lung, Skin, Eye, Breast, Kidney, Blood, Femur, Heart, Liver, Pancreas, Spleen) and modalities.
            <br>
<!--             <a href="https://github.com/PramitSaha/FOCUS-CVPR-2025" target="_blank"><b>[Code & Data Release]</b></a> -->
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- FL Settings, Datasets, and Tasks -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent; line-height: 1.5;">
          FL Settings, Datasets, and Tasks
        </h2>

        <div class="content" style="text-align: left; margin: auto; max-width: 800px;">
          <p>
            We evaluate <b>F<sup>3</sup>OCUS</b> for selective fine-tuning of layers and adapters across four vision-language models (ViLT, ALBEF, Llava-1.5, BLIP-2) and multiple federated learning settings:
          </p>
<ul>
  <li><b>Visual Question Answering:</b>
    <ul style="list-style: none; padding-left: 1em;">
      <li>(i) <b>Task 1:</b> 5 clients, domain gap (SLAKE, VQA-RAD, VQA-Med 2019/2020/2021)</li>
      <li>(ii) <b>Task 2:</b> 8 modality clients (CT, Ultrasound, etc.)</li>
      <li>(iii) <b>Task 3:</b> 9 clients on Ultra-MedVQA</li>
    </ul>
  </li>
  <li><b>Multi-label Disease Classification:</b>
    <ul style="list-style: none; padding-left: 1em;">
      <li>(iv) <b>Task 4:</b> 4 clients (Open-I)</li>
      <li>(v) <b>Task 5:</b> 10 clients (MIMIC), both with label shift (Dirichlet, 15 classes)</li>
    </ul>
  </li>
  <li><b>Heterogeneous Tasks:</b>
    <ul style="list-style: none; padding-left: 1em;">
      <li>(vi) <b>Task 6:</b> Combined VQA (3 clients) and disease-classification (2 clients)</li>
    </ul>
  </li>
</ul>

          <p>
            <b>Device Heterogeneity:</b> We vary the number of trainable layers per client/task to simulate diverse resource budgets.<br>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>


  

<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent; line-height: 1.5;">
          Performance Comparison with State-of-the-Art
        </h2>

        <div class="content" style="text-align: left; margin: auto; max-width: 800px;">
          <ul>
            <li>
              <b>PEFT Baselines:</b> F<sup>3</sup>OCUS outperforms popular parameter-efficient fine-tuning methods (LayerNorm Tuning, LoRA, Bias, Prompt, FedDAT) in both accuracy and efficiency, requiring fewer trainable parameters and lower computation.
            </li>
            <li>
              <b>Personalized FL Baselines:</b> Consistent gains over 12 SOTA personalized FL methods (e.g., perFedAvg, MetaFed, FedPAC, FLUTE, FedALA, FedProto, etc.) across five tasks.
            </li>
            <li>
              <b>Pruning Baselines:</b> Our LNTK-based selection surpasses classic pruning approaches (Federated Drop-out, Magnitude, FishMask, GraSP, SNIP, Synflow), with F<sup>3</sup>OCUS improving overall accuracy by up to 5.3% in both homogeneous and heterogeneous device settings.
            </li>
            <li>
              <b>Layer Selection Baselines:</b> LNTK and F<sup>3</sup>OCUS outperform adapter-drop, RGN, Fedselect, and SPT, with F<sup>3</sup>OCUS providing the best balance of accuracy, fairness, and efficiency.
            </li>
          </ul>
        </div>
        <figure style="text-align: center;">
          <img src="static/images/your_comparison_table.png" width="90%" alt="State-of-the-art comparison"/>
          <figcaption>Comparison with PEFT, personalized FL, pruning, and layer selection baselines. (Table from paper)</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>

<!-- Ablations & Visualizations -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-2" style="background: linear-gradient(to right, indigo, indigo, skyblue, indigo, indigo); -webkit-background-clip: text; -webkit-text-fill-color: transparent; line-height: 1.5;">
          Ablation Studies & Visualizations
        </h2>
        <div class="content" style="text-align: left; margin: auto; max-width: 800px;">
          <ul>
            <li>
              <b>Meta-Heuristics:</b> F<sup>3</sup>OCUS uses meta-heuristic optimization (NSGA, ABC, ACO, SA, MOPSO), which achieves strong performance across diverse tasks and clients.
            </li>
            <li>
              <b>Feature Visualization:</b> t-SNE plots show that F<sup>3</sup>OCUS generates more separable and discriminative features than pruning or standard PEFT.
            </li>
            <li>
              <b>Convergence:</b> Loss curves highlight faster and more stable training convergence for F<sup>3</sup>OCUS.
            </li>
            <li>
              <b>Layer Importance:</b> We visualize principal LNTK eigenvalues and selected layer ranks across all rounds, demonstrating how server-side optimization drives efficient, diverse adaptation.
            </li>
          </ul>
        </div>
        <figure style="text-align: center;">
          <img src="static/images/your_visualization_figure.png" width="90%" alt="Ablation and feature visualization"/>
          <figcaption>Example: t-SNE, loss curves, or layer rank visualizations. (See main paper figures)</figcaption>
        </figure>
      </div>
    </div>
  </div>
</section>
 -->


<!-- BibTeX -->
<section class="section hero">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
@inproceedings{saha2025f3ocus,
  title={F3OCUS: Federated Finetuning of Vision-Language Foundation Models with Optimal Client Layer Updating Strategy via Multi-objective Meta-Heuristics},
  author={Pramit Saha and Felix Wagner and Divyanshu Mishra and Can Peng and Anshul Thakur and David A. Clifton and Konstantinos Kamnitsas and J. Alison Noble},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
  year={2025}
}
    </code></pre>
  </div>
</section>

<!-- Acknowledgements 
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <h2 class="title">Acknowledgements</h2>
    <p>
      This research was supported by the UK Engineering and Physical Sciences Research Council (EPSRC), University of Oxford, and the Royal Academy of Engineering.  
      <br>
      Thanks to all collaborators, co-authors, and institutions for their contributions.
    </p>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Page built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> (adapted from <a href="https://nerfies.github.io" target="_blank">Nerfies</a>).
            <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer> -->

</body>
</html>

